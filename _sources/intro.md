# Self-Supervised Learning for Music Information Retrieval

Welcome to this comprehensive tutorial on Self-Supervised Learning (SSL) methods applied to Music Information Retrieval (MIR). This tutorial will guide you through the fundamental concepts, practical implementations, and cutting-edge new horizons for SSL.

## What You'll Learn

This tutorial covers:

- **Foundational concepts** of self-supervised learning and their application to music
- **Joint Embedding Architectures (JEA)** including contrastive learning, BYOL, and VICReg
- **Masked modeling** and Joint-Embedding Predictive Architectures (JEPA) for music
- **Equivariant SSL** for handling musical transformations like pitch and key changes
- **Generative SSL** approaches that combine representation learning with generation
- **Practical hands-on labs** with real code implementations
- **Evaluation methods** for SSL representations in music tasks


## Overview of Tutorial Structure and Code Environment

This tutorial is organized as a **Jupyter Book**, with chapters covering the main paradigms of SSL in MIR.  
Each chapter combines conceptual explanations, references to state-of-the-art works, and practical sessions with runnable code examples.  
Where relevant, we provide **placeholder figures** and code templates that will be progressively filled in during the hands-on sessions.  

You can navigate the tutorial as follows:

- **Chapter 1: Foundations** – A refresher on deep learning and the motivation for self-supervision  
- **Chapter 2: Joint Embedding Architectures (JEA)** – Contrastive learning, BYOL, VICReg, and their applications to music  
- **Chapter 3: Masked Modeling & JEPA** – Masked autoencoders, JEPA, and predictive approaches  
- **Chapter 4: Equivariant SSL** – Learning from musical transformations such as pitch shifting and key transposition  
- **Chapter 5: Generative SSL** – Exploring the interaction between representation learning and generative models  
- **Chapter 6: Wrapping Up** – Evaluating SSL representations and summarizing future challenges  
- **Chapter 7: Appendices** – Practical setup instructions and additional references  

> **Note:** All code will be provided in pre-prepared notebooks. Make sure to install the listed dependencies before attempting the hands-on parts.  

---

## Meet the Speakers

### [Julien Guinot](https://placeholder.link)
![Julien Guinot](path/to/julien_placeholder.png)  
Julien Guinot is a second-year PhD student at the **AI and Music Centre for Doctoral Training** at Queen Mary University of London, sponsored by Universal Music Group, under the supervision of Dr. György Fazekas, Dr. Emmanouil Benetos, and Dr. Elio Quinton. His research focuses on representation learning for music, with an emphasis on improving (multimodal) SSL approaches for user-centric applications such as controllable retrieval. His previous work on contrastive learning for music representations has been presented at **ISMIR** and **ICASSP**.

---

### [Alain Riou](https://placeholder.link)
![Alain Riou](path/to/alain_placeholder.png)  
Alain Riou is a recent PhD graduate who worked on self-supervised learning of musical representations at **Télécom Paris** and **Sony CSL – Paris**, under the supervision of Stefan Lattner, Gaëtan Hadjeres, and Geoffroy Peeters. His research interests include deep representation learning and SSL for MIR. His work *PESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective* received the **Best Paper Award at ISMIR 2023**. More recently, his work on JEPA models for music has been accepted at ISMIR and ICASSP.

---

### [Yuexuan Kong](https://placeholder.link)
![Yuexuan Kong](path/to/yuexuan_placeholder.png)  
Yuexuan Kong is a second-year industrial PhD student at **Deezer** and **LS2N (CNRS)** at Ecole Centrale de Nantes, under the supervision of Dr. Gabriel Meseguer-Brocal, Dr. Vincent Lostanlen, Dr. Mathieu Lagrange, and Dr. Romain Hennequin. Her research focuses on self-supervised learning for music, with a particular emphasis on **equivariant SSL** and contrastive learning.

---

### [Marco Pasini](https://placeholder.link)
![Marco Pasini](path/to/marco_placeholder.png)  
Marco Pasini is a second-year PhD student at **Queen Mary University of London**, in collaboration with **Sony Computer Science Laboratories – Paris**. He works on **Generative Modeling** applied to audio, including projects such as **Musika** for fast music generation, the **Music2Latent** series for efficient audio compression, **Diff-A-Riff** for accompaniment generation, and the **Continuous Autoregressive Models** framework.

---

### [Gabriel Meseguer-Brocal](https://placeholder.link)
![Gabriel Meseguer-Brocal](path/to/gabriel_placeholder.png)  
Gabriel Meseguer-Brocal is a research scientist at **Deezer**. Previously, he completed postdoctoral research at **CNRS** in France and earned his PhD in 2020 at **IRCAM**. His research interests include signal processing and deep learning for music, with applications in **source separation, dataset creation, multi-tagging, self-supervised learning, and multimodal analysis**.

---

### [Stefan Lattner](https://placeholder.link)
![Stefan Lattner](path/to/stefan_placeholder.png)  
Stefan Lattner is a research leader at the **Sony CSL – Paris music team**, working on generative AI for music production, MIR, and computational music perception. He earned his PhD in 2019 from **Johannes Kepler University Linz**, focusing on modeling musical structure, transformation learning, and computational relative pitch perception. His interests span **generative sequence models, representation learning, and human-computer interaction in music creation**. He received the **Best Paper Award at ISMIR 2019** for his work *Learning Complex Basis Functions for Invariant Representations of Audio*.  

---

